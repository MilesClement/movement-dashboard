{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.display import GeoJSON\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import urllib.request\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Targeted baseline of Nov-Feb\n",
    "sensorListNov = ['PER_AIRMON_MONITOR1048100', \n",
    "                'PER_AIRMON_MONITOR1056100',\n",
    "                'PER_AIRMON_MONITOR914', \n",
    "                'PER_AIRMON_MONITOR1135100']\n",
    "\n",
    "## Some sensors don't have data for November, different loop used for the below\n",
    "sensorListDec = ['PER_AIRMON_MONITOR1156100', \n",
    "                'PER_AIRMON_MONITOR1155100']\n",
    "\n",
    "variables = ['NO2','PM2.5','PM10']\n",
    "\n",
    "NE_Comb_Baseline = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sensorListNov:\n",
    "    for v in variables:\n",
    "        ## Pull Nov, Dec, Jan from cache\n",
    "        r = requests.get(\"http://uoweb3.ncl.ac.uk/api/v1.1/sensors/\"+s+\"/data/cached/\"+v+\"/2019/11/csv/\")\n",
    "        sensorNov2019 = pd.read_csv(io.StringIO(r.text))\n",
    "        r = requests.get(\"http://uoweb3.ncl.ac.uk/api/v1.1/sensors/\"+s+\"/data/cached/\"+v+\"/2019/12/csv/\")\n",
    "        sensorDec2019 = pd.read_csv(io.StringIO(r.text))\n",
    "        r = requests.get(\"http://uoweb3.ncl.ac.uk/api/v1.1/sensors/\"+s+\"/data/cached/\"+v+\"/2020/1/csv/\")\n",
    "        sensorJan2020 = pd.read_csv(io.StringIO(r.text))\n",
    "        ## Pull Feb from 'live' databases\n",
    "        dataCall = 'http://uoweb3.ncl.ac.uk/api/v1.1/sensors/'+s+'/data/csv/'\n",
    "        params = dict(\n",
    "            starttime='20200201',\n",
    "            endtime='20200301',\n",
    "            data_variable=v\n",
    "        )\n",
    "        r = requests.get(dataCall,params)\n",
    "        sensorFeb2020 = pd.read_csv(io.StringIO(r.text))\n",
    "        ## Join monthly dataframes\n",
    "        sensorData = pd.concat([sensorNov2019,sensorDec2019, sensorJan2020,sensorFeb2020])\n",
    "        ## Copy Raw Data into NE Combined dataset\n",
    "        NE_Comb_Baseline = NE_Comb_Baseline.append(sensorData, ignore_index=True)\n",
    "        ## Remove flagged data, create weekday and hour columns, remove unneeded columns\n",
    "        sensorData = sensorData[sensorData[\"Flagged as Suspect Reading\"]==False]\n",
    "        sensorData.loc[:,\"Weekday\"] = pd.to_datetime(sensorData['Timestamp']).dt.weekday\n",
    "        sensorData.loc[:,\"Hour\"] = pd.to_datetime(sensorData['Timestamp']).dt.hour\n",
    "        sensorData.drop(columns=['Flagged as Suspect Reading', \n",
    "                                 'Location (WKT)',\n",
    "                                 'Ground Height Above Sea Level',\n",
    "                                 'Sensor Height Above Ground',\n",
    "                                 'Broker Name',\n",
    "                                 'Third Party',\n",
    "                                 'Sensor Centroid Longitude',\n",
    "                                 'Sensor Centroid Latitude',\n",
    "                                 'Raw ID','Sensor Name','Variable','Units','Timestamp'], axis=1, inplace=True)\n",
    "        ## Aggregate baseline data by day/hour - median, q15, q85\n",
    "        aggregateColumns = ['Weekday', 'Hour']\n",
    "        dfBaseMean = sensorData.groupby(aggregateColumns, group_keys=False, as_index=False).median()\n",
    "        ## Currently 15/85th Percentiles to match Luke Smith Plots\n",
    "        dfBaseLQ = sensorData.groupby(aggregateColumns, group_keys=False, as_index=False).quantile(.15)\n",
    "        dfBaseHQ = sensorData.groupby(aggregateColumns, group_keys=False, as_index=False).quantile(.85)\n",
    "        ## Comb aggregates into final dataframe\n",
    "        dfComb = pd.DataFrame(columns=[\"Weekday\",\"Hour\",\"Mean\",\"LQ\",\"HQ\"])\n",
    "        dfComb[\"Weekday\"] = dfBaseMean[\"Weekday\"]\n",
    "        dfComb[\"Hour\"] = dfBaseMean[\"Hour\"]\n",
    "        dfComb[\"Mean\"] = dfBaseMean[\"Value\"]\n",
    "        dfComb[\"LQ\"] = dfBaseLQ[\"Value\"]\n",
    "        dfComb[\"HQ\"] = dfBaseHQ[\"Value\"]\n",
    "        ##Pickle out - not used pickle before, need to test -  change to cache when ready\n",
    "        pd.to_pickle(dfComb, 'C:/Covid_Baseline/baseline_aq_'+s+'_'+v+'.pkl')\n",
    "        dfComb = None\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sensorListDec:\n",
    "    for v in variables:\n",
    "        ## Pull Dec, Jan from cache\n",
    "        r = requests.get(\"http://uoweb3.ncl.ac.uk/api/v1.1/sensors/\"+s+\"/data/cached/\"+v+\"/2019/12/csv/\")\n",
    "        sensorDec2019 = pd.read_csv(io.StringIO(r.text))\n",
    "        r = requests.get(\"http://uoweb3.ncl.ac.uk/api/v1.1/sensors/\"+s+\"/data/cached/\"+v+\"/2020/1/csv/\")\n",
    "        sensorJan2020 = pd.read_csv(io.StringIO(r.text))\n",
    "        ## Pull Feb from 'live' databases\n",
    "        dataCall = 'http://uoweb3.ncl.ac.uk/api/v1.1/sensors/'+s+'/data/csv/'\n",
    "        params = dict(\n",
    "            starttime='20200201',\n",
    "            endtime='20200301',\n",
    "            data_variable=v\n",
    "        )\n",
    "        r = requests.get(dataCall,params)\n",
    "        sensorFeb2020 = pd.read_csv(io.StringIO(r.text))\n",
    "        ## Join monthly dataframes\n",
    "        sensorData = pd.concat([sensorDec2019, sensorJan2020,sensorFeb2020])\n",
    "        ## Copy Raw Data into NE Combined dataset\n",
    "        NE_Comb_Baseline = NE_Comb_Baseline.append(sensorData, ignore_index=True)\n",
    "        ## Remove flagged data, create weekday and hour columns, remove unneeded columns\n",
    "        sensorData = sensorData[sensorData[\"Flagged as Suspect Reading\"]==False]\n",
    "        sensorData.loc[:,\"Weekday\"] = pd.to_datetime(sensorData['Timestamp']).dt.weekday\n",
    "        sensorData.loc[:,\"Hour\"] = pd.to_datetime(sensorData['Timestamp']).dt.hour\n",
    "        sensorData.drop(columns=['Flagged as Suspect Reading', \n",
    "                                 'Location (WKT)',\n",
    "                                 'Ground Height Above Sea Level',\n",
    "                                 'Sensor Height Above Ground',\n",
    "                                 'Broker Name',\n",
    "                                 'Third Party',\n",
    "                                 'Sensor Centroid Longitude',\n",
    "                                 'Sensor Centroid Latitude',\n",
    "                                 'Raw ID','Sensor Name','Variable','Units','Timestamp'], axis=1, inplace=True)\n",
    "        ## Aggregate baseline data by day/hour - median, q15, q85\n",
    "        aggregateColumns = ['Weekday', 'Hour']\n",
    "        dfBaseMean = sensorData.groupby(aggregateColumns, group_keys=False, as_index=False).median()\n",
    "        dfBaseLQ = sensorData.groupby(aggregateColumns, group_keys=False, as_index=False).quantile(.15)\n",
    "        dfBaseHQ = sensorData.groupby(aggregateColumns, group_keys=False, as_index=False).quantile(.85)\n",
    "        ## Comb aggregates into final dataframe\n",
    "        dfComb = pd.DataFrame(columns=[\"Weekday\",\"Hour\",\"Mean\",\"LQ\",\"HQ\"])\n",
    "        dfComb[\"Weekday\"] = dfBaseMean[\"Weekday\"]\n",
    "        dfComb[\"Hour\"] = dfBaseMean[\"Hour\"]\n",
    "        dfComb[\"Mean\"] = dfBaseMean[\"Value\"]\n",
    "        dfComb[\"LQ\"] = dfBaseLQ[\"Value\"]\n",
    "        dfComb[\"HQ\"] = dfBaseHQ[\"Value\"]\n",
    "        ##Pickle out - not used pickle before, need to test - change to cache when ready\n",
    "        pd.to_pickle(dfComb, 'C:/Covid_Baseline/baseline_aq_'+s+'_'+v+'.pkl')\n",
    "        dfComb = None\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in variables:\n",
    "    dfTemp = NE_Comb_Baseline[NE_Comb_Baseline[\"Variable\"]==v]\n",
    "    \n",
    "    dfTemp = dfTemp[dfTemp[\"Flagged as Suspect Reading\"]==False]\n",
    "    dfTemp.loc[:,\"Weekday\"] = pd.to_datetime(dfTemp['Timestamp']).dt.weekday\n",
    "    dfTemp.loc[:,\"Hour\"] = pd.to_datetime(dfTemp['Timestamp']).dt.hour\n",
    "    dfTemp.drop(columns=['Flagged as Suspect Reading', \n",
    "                                 'Location (WKT)',\n",
    "                                 'Ground Height Above Sea Level',\n",
    "                                 'Sensor Height Above Ground',\n",
    "                                 'Broker Name',\n",
    "                                 'Third Party',\n",
    "                                 'Sensor Centroid Longitude',\n",
    "                                 'Sensor Centroid Latitude',\n",
    "                                 'Raw ID','Sensor Name','Variable','Units','Timestamp'], axis=1, inplace=True)\n",
    "    ## Aggregate baseline data by day/hour - median, q15, q85\n",
    "    aggregateColumns = ['Weekday', 'Hour']\n",
    "    dfBaseMean = dfTemp.groupby(aggregateColumns, group_keys=False, as_index=False).median()\n",
    "    dfBaseLQ = dfTemp.groupby(aggregateColumns, group_keys=False, as_index=False).quantile(.15)\n",
    "    dfBaseHQ = dfTemp.groupby(aggregateColumns, group_keys=False, as_index=False).quantile(.85)\n",
    "    ## Comb aggregates into final dataframe\n",
    "    dfComb = pd.DataFrame(columns=[\"Weekday\",\"Hour\",\"Mean\",\"LQ\",\"HQ\"])\n",
    "    dfComb[\"Weekday\"] = dfBaseMean[\"Weekday\"]\n",
    "    dfComb[\"Hour\"] = dfBaseMean[\"Hour\"]\n",
    "    dfComb[\"Mean\"] = dfBaseMean[\"Value\"]\n",
    "    dfComb[\"LQ\"] = dfBaseLQ[\"Value\"]\n",
    "    dfComb[\"HQ\"] = dfBaseHQ[\"Value\"]\n",
    "    ##Pickle out - not used pickle before, need to test - change to cache when ready\n",
    "    pd.to_pickle(dfComb, 'C:/Covid_Baseline/NE_Aggregated_'+v+'_Baseline.pkl')\n",
    "    dfComb = None\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
